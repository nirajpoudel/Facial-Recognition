{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Facial Recognition_VGG16.ipynb","provenance":[],"mount_file_id":"1rXt9ufWfSNLcQ87oFPmY72R9v8vTwhdx","authorship_tag":"ABX9TyOUeaUWcF04WfIDqALvIWgs"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"5DEv8IFecayi","executionInfo":{"status":"ok","timestamp":1614567681443,"user_tz":-345,"elapsed":1018,"user":{"displayName":"Niraj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAcL5rAnMjJOT8ESeQyjVl6m_tQSW2HaqUoUfMSA=s64","userId":"16495485645589276387"}}},"source":["from keras.layers import Input, Lambda, Dense, Flatten\n","from keras.models import Model\n","from keras.applications.vgg16 import VGG16\n","from keras.applications.vgg16 import preprocess_input\n","from keras.preprocessing import image\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.models import Sequential\n","import numpy as np\n","from glob import glob\n","import matplotlib.pyplot as plt\n","from keras.preprocessing.image import ImageDataGenerator\n","import tensorflow as tf\n","from keras.models import load_model\n","from IPython.display import Image"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"JZY5IaZPhG9O"},"source":["IMAGE_SIZE = [224, 224]\n","train_path = './drive/MyDrive/Facial Recognition/Datasets/train'\n","valid_path = './drive/MyDrive/Facial Recognition/Datasets/test'\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QzaJlTmnlbaX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614564830974,"user_tz":-345,"elapsed":2257,"user":{"displayName":"Niraj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAcL5rAnMjJOT8ESeQyjVl6m_tQSW2HaqUoUfMSA=s64","userId":"16495485645589276387"}},"outputId":"a0b2c806-6f85-4f3a-faaa-c4d921eb82cb"},"source":["# add preprocessing layer to the front of VGG\n","vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58892288/58889256 [==============================] - 1s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XtklHtlxSS-a"},"source":["# don't train existing weights\n","for layer in vgg.layers:\n","  layer.trainable = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9cwaICSxSXrC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614564835576,"user_tz":-345,"elapsed":1380,"user":{"displayName":"Niraj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAcL5rAnMjJOT8ESeQyjVl6m_tQSW2HaqUoUfMSA=s64","userId":"16495485645589276387"}},"outputId":"ed6b4d93-5aac-4e10-89fa-b3e540642de5"},"source":["# useful for getting number of classes\n","folders = glob('./drive/MyDrive/Facial Recognition/Datasets/train/*')\n","folders"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['./drive/MyDrive/Facial Recognition/Datasets/train/Newsun',\n"," './drive/MyDrive/Facial Recognition/Datasets/train/Niraj']"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"xPBDJozZSzWa"},"source":["x = Flatten()(vgg.output)\n","# x = Dense(1000, activation='relu')(x)\n","prediction = Dense(len(folders), activation='softmax')(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AZ_taZZ-S9Of","executionInfo":{"status":"ok","timestamp":1614564842914,"user_tz":-345,"elapsed":993,"user":{"displayName":"Niraj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAcL5rAnMjJOT8ESeQyjVl6m_tQSW2HaqUoUfMSA=s64","userId":"16495485645589276387"}},"outputId":"c975f019-af64-4274-abfe-b761cdf7544b"},"source":["# create a model object\n","model = Model(inputs=vgg.input, outputs=prediction)\n","# view the structure of the model\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 25088)             0         \n","_________________________________________________________________\n","dense (Dense)                (None, 2)                 50178     \n","=================================================================\n","Total params: 14,764,866\n","Trainable params: 50,178\n","Non-trainable params: 14,714,688\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kN8qD0U8S_5r"},"source":["# tell the model what cost and optimization method to use\n","model.compile(\n","  loss='categorical_crossentropy',\n","  optimizer='adam',\n","  metrics=['accuracy']\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Li8TUF_cTF8-","executionInfo":{"status":"ok","timestamp":1614564846718,"user_tz":-345,"elapsed":1299,"user":{"displayName":"Niraj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAcL5rAnMjJOT8ESeQyjVl6m_tQSW2HaqUoUfMSA=s64","userId":"16495485645589276387"}},"outputId":"e4f918ee-2966-489c-9dc2-d03b76c1a170"},"source":["train_datagen = ImageDataGenerator(rescale = 1./255,\n","                                   shear_range = 0.2,\n","                                   zoom_range = 0.2,\n","                                   horizontal_flip = True)\n","\n","test_datagen = ImageDataGenerator(rescale = 1./255)\n","\n","training_set = train_datagen.flow_from_directory('./drive/MyDrive/Facial Recognition/Datasets/train',\n","                                                 target_size = (224, 224),\n","                                                 batch_size = 32,\n","                                                 class_mode = 'categorical')\n","\n","test_set = test_datagen.flow_from_directory('./drive/MyDrive/Facial Recognition/Datasets/test',\n","                                            target_size = (224, 224),\n","                                            batch_size = 32,\n","                                            class_mode = 'categorical')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 200 images belonging to 2 classes.\n","Found 60 images belonging to 2 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4g1SQIhoTKwQ","executionInfo":{"status":"ok","timestamp":1614565935621,"user_tz":-345,"elapsed":724523,"user":{"displayName":"Niraj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAcL5rAnMjJOT8ESeQyjVl6m_tQSW2HaqUoUfMSA=s64","userId":"16495485645589276387"}},"outputId":"8b7f744f-4974-43bc-8e59-ec42833d4043"},"source":["# fit the model\n","r = model.fit_generator(\n","  training_set,\n","  validation_data=test_set,\n","  epochs=5,\n","  steps_per_epoch=len(training_set),\n","  validation_steps=len(test_set)\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  warnings.warn('`Model.fit_generator` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/5\n","7/7 [==============================] - 185s 25s/step - loss: 1.0343 - accuracy: 0.5347 - val_loss: 0.7062 - val_accuracy: 0.5667\n","Epoch 2/5\n","7/7 [==============================] - 130s 19s/step - loss: 0.2212 - accuracy: 0.9180 - val_loss: 0.1226 - val_accuracy: 0.9833\n","Epoch 3/5\n","7/7 [==============================] - 130s 19s/step - loss: 0.0695 - accuracy: 0.9823 - val_loss: 0.0832 - val_accuracy: 1.0000\n","Epoch 4/5\n","7/7 [==============================] - 130s 19s/step - loss: 0.0401 - accuracy: 0.9757 - val_loss: 0.5918 - val_accuracy: 0.7167\n","Epoch 5/5\n","7/7 [==============================] - 130s 19s/step - loss: 0.0286 - accuracy: 1.0000 - val_loss: 0.0807 - val_accuracy: 1.0000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LHnge9gvVBG8"},"source":["model.save('./drive/MyDrive/Facial Recognition/VGG16_model.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oz1hv8gUVcKp"},"source":[""],"execution_count":null,"outputs":[]}]}